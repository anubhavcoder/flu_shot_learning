{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change viewing options\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "train = pd.read_csv(\"training_set_features.csv\")\n",
    "train_labels = pd.read_csv(\"training_set_labels.csv\")\n",
    "test = pd.read_csv(\"test_set_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Do data preprocessing, scaling, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data exploration phase, we decided which columns were relevant to our analysis. \n",
    "\n",
    "So, we'll subset the training and test sets by these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns to subset\n",
    "columns_subset = ['h1n1_concern', 'h1n1_knowledge', 'doctor_recc_h1n1', 'doctor_recc_seasonal', \n",
    "                 'chronic_med_condition', 'opinion_h1n1_vacc_effective', 'opinion_h1n1_risk', \n",
    "                 'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective', 'opinion_seas_risk', \n",
    "                 'opinion_seas_sick_from_vacc', 'age_group', 'education', 'income_poverty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset datasets\n",
    "train_subset = train.loc[:, columns_subset]\n",
    "test_subset = test.loc[:, columns_subset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to do something about any missing data that we observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of NaNs, by column, in the training set, is: \n",
      "\n",
      "h1n1_concern                   92  \n",
      "h1n1_knowledge                 116 \n",
      "doctor_recc_h1n1               2160\n",
      "doctor_recc_seasonal           2160\n",
      "chronic_med_condition          971 \n",
      "opinion_h1n1_vacc_effective    391 \n",
      "opinion_h1n1_risk              388 \n",
      "opinion_h1n1_sick_from_vacc    395 \n",
      "opinion_seas_vacc_effective    462 \n",
      "opinion_seas_risk              514 \n",
      "opinion_seas_sick_from_vacc    537 \n",
      "age_group                      0   \n",
      "education                      1407\n",
      "income_poverty                 4423\n",
      "dtype: int64 \n",
      "\n",
      " =========\n",
      "The number of NaNs, by column, in the test set, is: \n",
      "\n",
      "h1n1_concern                   85  \n",
      "h1n1_knowledge                 122 \n",
      "doctor_recc_h1n1               2160\n",
      "doctor_recc_seasonal           2160\n",
      "chronic_med_condition          932 \n",
      "opinion_h1n1_vacc_effective    398 \n",
      "opinion_h1n1_risk              380 \n",
      "opinion_h1n1_sick_from_vacc    375 \n",
      "opinion_seas_vacc_effective    452 \n",
      "opinion_seas_risk              499 \n",
      "opinion_seas_sick_from_vacc    521 \n",
      "age_group                      0   \n",
      "education                      1407\n",
      "income_poverty                 4497\n",
      "dtype: int64 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loop through all columns, count NaNs\n",
    "print(f\"The number of NaNs, by column, in the training set, is: \\n\\n{train_subset.isna().sum()} \\n\\n =========\")\n",
    "print(f\"The number of NaNs, by column, in the test set, is: \\n\\n{test_subset.isna().sum()} \\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the survey data, we can likely impute the NaNs with either 0/unknown or the mean/median values. For the demographic values that are unknown (education, income_poverty), we can just impute them with 'unknown'. Let's check the dtypes of each column. If they're all categorical/object, we can just impute missing data with \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dtype of the h1n1_concern column is: <class 'numpy.float64'>\n",
      "=======\n",
      "The dtype of the h1n1_knowledge column is: <class 'numpy.float64'>\n",
      "=======\n",
      "The dtype of the doctor_recc_h1n1 column is: <class 'numpy.float64'>\n",
      "=======\n",
      "The dtype of the doctor_recc_seasonal column is: <class 'numpy.float64'>\n",
      "=======\n",
      "The dtype of the chronic_med_condition column is: <class 'numpy.float64'>\n",
      "=======\n",
      "The dtype of the opinion_h1n1_vacc_effective column is: <class 'numpy.float64'>\n",
      "=======\n",
      "The dtype of the opinion_h1n1_risk column is: <class 'numpy.float64'>\n",
      "=======\n",
      "The dtype of the opinion_h1n1_sick_from_vacc column is: <class 'numpy.float64'>\n",
      "=======\n",
      "The dtype of the opinion_seas_vacc_effective column is: <class 'numpy.float64'>\n",
      "=======\n",
      "The dtype of the opinion_seas_risk column is: <class 'numpy.float64'>\n",
      "=======\n",
      "The dtype of the opinion_seas_sick_from_vacc column is: <class 'numpy.float64'>\n",
      "=======\n",
      "The dtype of the age_group column is: <class 'str'>\n",
      "=======\n",
      "The dtype of the education column is: <class 'str'>\n",
      "=======\n",
      "The dtype of the income_poverty column is: <class 'str'>\n",
      "=======\n"
     ]
    }
   ],
   "source": [
    "# for training columns (test columns will be the same)\n",
    "for col in train_subset.columns:\n",
    "    print(f\"The dtype of the {col} column is: {type(train_subset[col][0])}\\n=======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the columns are a mix of ints and strings, we can split the data, impute each separately, then re-append them to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split values\n",
    "train_subset_numeric = train_subset.loc[:, ['h1n1_concern', 'h1n1_knowledge', 'doctor_recc_h1n1', 'doctor_recc_seasonal', \n",
    "                 'chronic_med_condition', 'opinion_h1n1_vacc_effective', 'opinion_h1n1_risk', \n",
    "                 'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective', 'opinion_seas_risk', \n",
    "                 'opinion_seas_sick_from_vacc']]\n",
    "\n",
    "train_subset_str = train_subset.loc[:, [ 'age_group', 'education', 'income_poverty']]\n",
    "\n",
    "test_subset_numeric = test_subset.loc[:, ['h1n1_concern', 'h1n1_knowledge', 'doctor_recc_h1n1', 'doctor_recc_seasonal', \n",
    "                 'chronic_med_condition', 'opinion_h1n1_vacc_effective', 'opinion_h1n1_risk', \n",
    "                 'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective', 'opinion_seas_risk', \n",
    "                 'opinion_seas_sick_from_vacc']]\n",
    "\n",
    "test_subset_str = test_subset.loc[:, [ 'age_group', 'education', 'income_poverty']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the numeric values, there are several strategies that we could use:\n",
    "\n",
    "1. Impute with mean/median: This would be the simplest strategy, but might create class imbalances and skew the results by artificially inflating some counts\n",
    "2. Impute with predicted value (e.g., fit a logistic regression, and use the prediction as the impute value): this would probably be the most robust method, since it uses existing data to make a prediction on what the true values should be. But, this method is imperfect because we also have other missing data in other categories as well (so, to fit a logistic regression, we'd have to impute these other missing values) and we also don't know how reliable our imputations would be. \n",
    "3. Impute randomly, such that at the end, the class proportions remain the same. This method would keep the balance in proportions consistent, but at the cost of introducing noise by randomly assigning class values to observations. \n",
    "4. Filter out missing data.\n",
    "\n",
    "Personally, I lean towards using a logistic regression to predict what the missing values should be, since it seems to be the method that will best maintain existing class proportions while accounting for the other features of the data (so, two people who have the same values for every other column in the dataset should presumably have the same value for the column in question)\n",
    "\n",
    "But, before doing so, it's worth checking to see how \"similar\" the missing people are to those who we have data for. If they're similar, it would make sense to use information from the existing data in order to impute the missing data. Otherwise, it might be advisable to adapt the imputation method or filter them out entirely. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit separate imputer objects on them\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
